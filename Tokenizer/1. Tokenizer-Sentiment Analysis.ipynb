{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "017d10fd",
   "metadata": {},
   "source": [
    "## Tokenizer 构造输入\n",
    "\n",
    "- 调用模型: distilbert-base-uncased-finetuned-sst-2-english\n",
    "- link: https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english\n",
    "\n",
    "\n",
    "**Tokenizer需要和Model配合使用：一定要相匹配**\n",
    "\n",
    "- Tokenizer outputs => model input\n",
    "\n",
    "- Auto\\*Tokenizer  & Auto\\*Model: Generic type\n",
    "\n",
    "\n",
    "- tokenizer: 完全服务于model input\n",
    "    - len(input_ids) == len(attention_mask)\n",
    "    - tokenizer(test_sentence): 实际上实在内部调用tokenizer.\\_\\_call\\_\\_: encode\n",
    "    - tokenizer.encode == tokenizer.tokenize + tokenizer.convert_tokens_to_ids\n",
    "    - tokenizer.decode\n",
    "    - tokenizer 的工作原理: tokenizer.vocab: 字典 token => ids 的映射关系\n",
    "        - tokenizer.special_token_maps\n",
    "    - attention_mask == 1 不是padding的部分 == 0 是padding的部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0f2cb34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = ['today is not that bad', 'today is so bad', 'I don\\'t know how to do it!']\n",
    "model_name = 'distilbert-base-uncased-finetuned-sst-2-english'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "51eba622",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "af7ac35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入tokenizer和模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "964a547d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='distilbert-base-uncased-finetuned-sst-2-english', vocab_size=30522, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2e624324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2651, 2003, 2025, 2008, 2919,  102,    0,    0,    0,    0,    0],\n",
       "        [ 101, 2651, 2003, 2061, 2919,  102,    0,    0,    0,    0,    0,    0],\n",
       "        [ 101, 1045, 2123, 1005, 1056, 2113, 2129, 2000, 2079, 2009,  999,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_input = tokenizer(test_sentence, truncation=True, padding=True, return_tensors='pt')\n",
    "batch_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ca847add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1045, 2123, 1005, 1056, 2113, 2129, 2000, 2079, 2009, 999, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(test_sentence[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "aec15575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 1045, 2123, 1005, 1056, 2113, 2129, 2000, 2079, 2009, 999, 102]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(test_sentence[2])   # 102是一个结束词 101是起始词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e01814f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'don', \"'\", 't', 'know', 'how', 'to', 'do', 'it', '!']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(test_sentence[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d16991b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2651, 2003, 2025, 2008, 2919]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(tokenizer.tokenize(test_sentence[0])) #[2651, 2003, 7293, 2008, 2919] 与encode对应一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "02f9a3bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] today is nod that bad [SEP]'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([101, 2651, 2003, 7293, 2008, 2919, 102])  # [CLS] [SEP] Bert经典分词方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2baa3b40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3fb1b55a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'unk_token': '[UNK]',\n",
       " 'sep_token': '[SEP]',\n",
       " 'pad_token': '[PAD]',\n",
       " 'cls_token': '[CLS]',\n",
       " 'mask_token': '[MASK]'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7dd49e83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100, 102, 0, 101, 103]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(sprcial_token for sprcial_token in tokenizer.special_tokens_map.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8259d9df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2651, 2003, 2025, 2008, 2919,  102,    0,    0,    0,    0,    0],\n",
       "        [ 101, 2651, 2003, 2061, 2919,  102,    0,    0,    0,    0,    0,    0],\n",
       "        [ 101, 1045, 2123, 1005, 1056, 2113, 2129, 2000, 2079, 2009,  999,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max_length 确定序列的最大长度\n",
    "# truncation 不符合最大长度进行剪切\n",
    "# padding 补充padding补齐\n",
    "# return_tensors 使用pytorch\n",
    "\n",
    "# 如果padding等于True的情况下会忽略掉max_length, 同理，如果padding='max_length', 那么序列最大长度都会padidng补齐, 0\n",
    "batch_input = tokenizer(test_sentence, max_length=256, truncation=True, padding=True, return_tensors='pt')\n",
    "batch_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e74ae840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2651, 2003, 2025, 2008, 2919,  102,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [ 101, 2651, 2003, 2061, 2919,  102,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [ 101, 1045, 2123, 1005, 1056, 2113, 2129, 2000, 2079, 2009,  999,  102,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_input = tokenizer(test_sentence, max_length=32, truncation=True, padding='max_length', return_tensors='pt')\n",
    "batch_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e9c28e",
   "metadata": {},
   "source": [
    "## Model 模型调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "13281771",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "46b5c877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[-3.4620,  3.6118],\n",
      "        [ 4.7508, -3.7899],\n",
      "        [ 4.4198, -3.5049]]), hidden_states=None, attentions=None)\n",
      "tensor([[8.4632e-04, 9.9915e-01],\n",
      "        [9.9980e-01, 1.9531e-04],\n",
      "        [9.9964e-01, 3.6156e-04]])\n",
      "tensor([1, 0, 0])\n",
      "['POSITIVE', 'NEGATIVE', 'NEGATIVE']\n"
     ]
    }
   ],
   "source": [
    "# 取消梯度\n",
    "# scores是经过softmax之后的情绪分类的归一划参数，然后使用argmax函数取一行中的最大值的索引，代表正向或者反向\n",
    "with torch.no_grad():\n",
    "    outputs = model(**batch_input)\n",
    "    print(outputs)\n",
    "    scores = F.softmax(outputs.logits, dim = 1)\n",
    "    print(scores)\n",
    "    labels = torch.argmax(scores, dim = 1)\n",
    "    print(labels)\n",
    "    label = [model.config.id2label[sentiment] for sentiment in labels.tolist()]\n",
    "    print(label)  # ['POSITIVE', 'NEGATIVE', 'NEGATIVE']\n",
    "    \n",
    "# ['today is not that bad', 'today is so bad', 'I don\\'t know how to do it!'] pos neg neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0b6274b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertConfig {\n",
       "  \"_name_or_path\": \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
       "  \"activation\": \"gelu\",\n",
       "  \"architectures\": [\n",
       "    \"DistilBertForSequenceClassification\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.1,\n",
       "  \"dim\": 768,\n",
       "  \"dropout\": 0.1,\n",
       "  \"finetuning_task\": \"sst-2\",\n",
       "  \"hidden_dim\": 3072,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"NEGATIVE\",\n",
       "    \"1\": \"POSITIVE\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"label2id\": {\n",
       "    \"NEGATIVE\": 0,\n",
       "    \"POSITIVE\": 1\n",
       "  },\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"distilbert\",\n",
       "  \"n_heads\": 12,\n",
       "  \"n_layers\": 6,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"qa_dropout\": 0.1,\n",
       "  \"seq_classif_dropout\": 0.2,\n",
       "  \"sinusoidal_pos_embds\": false,\n",
       "  \"tie_weights_\": true,\n",
       "  \"transformers_version\": \"4.25.1\",\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
