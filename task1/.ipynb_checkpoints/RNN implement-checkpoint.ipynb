{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "897b8492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 6])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(45)\n",
    "# 默认batch_size为1，只需要后续进行升维就可以unsqueeze()\n",
    "input_size, sequence_length = 6, 4 # input_szie 特征大小为6  sequence_length 词元序列长度为4\n",
    "input = torch.randn(sequence_length, input_size).unsqueeze(0) # 设置单词的长度为4，特征向量的维度是6，并且提升第0维度\n",
    "# 此时的input_size的维度为：N*L*d 对应batch_first = True\n",
    "input.shape  #N*L*d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3300ce63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 初始化隐藏层\n",
    "# h_0 为初始时刻的隐状态。当RNN为单向RNN时，h_0 的形状应为 num_layers × N × h ；当RNN为双向RNN时，h_0 的形状应为 ( 2 ⋅ num_layers ) × N × h。如不提供该参数的值，则默认为全0张量。\n",
    "hidden_size = 3 \n",
    "h_prev = torch.zeros(hidden_size).unsqueeze(0)  # 这里初始化的就是单层，h层数，N样batch_size\n",
    "h_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bdef1ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.8493, -0.5992, -0.2526],\n",
       "          [-0.6106, -0.1691,  0.5180],\n",
       "          [ 0.6820,  0.9631,  0.0999],\n",
       "          [ 0.0734, -0.5295,  0.9324]]], grad_fn=<TransposeBackward1>),\n",
       " tensor([[[ 0.0734, -0.5295,  0.9324]]], grad_fn=<StackBackward0>))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 调用pytorch的RNN api来验证结果\n",
    "rnn = nn.RNN(input_size, hidden_size, batch_first=True) # 传入两个参数：特征大小input_size和隐藏层数hidden_size\n",
    "rnn_output, state_final = rnn(input, h_prev)  # 返回两个结果，一个是output，另一个是最后时刻的h的值\n",
    "print(\"PyTorch API output:\")\n",
    "print(rnn_output)\n",
    "print(state_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "95342180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 手写RNN_forward函数，实现RNN的计算原理（单向单层）\n",
    "def rnn_forward(input, weight_ih, bias_ih, weight_hh, bias_hh, h_prev):\n",
    "    # input的shape对应三个参数：batch_size, sequence_length, input_size\n",
    "    batch_size, sequence_length, input_size = input.shape\n",
    "    # h_dim代表隐藏层的维度，也就是隐藏层的层数 [0]是因为矩阵相乘第一个维度作为不抵消的维度\n",
    "    h_dim = weight_ih.shape[0]  \n",
    "    # 初始化一个状态（输出）矩阵\n",
    "    h_out = torch.zeros(batch_size, sequence_length, h_dim) # h_dim就是hidden_size\n",
    "    \n",
    "    for t in range(sequence_length):\n",
    "        x = input[:, t, :]  # 获取当前时刻的输入（每一个时刻对应的是一个词元向量）\n",
    "        # x 的默认大小应该是 batch_size * input_size\n",
    "        # weight_ih 的默认大小应该是 h_dim * input_size，所以要进行weight_ih的扩充，变成 batch_size*h_dim*input_size\n",
    "        weight_ih_batch = weight_ih.unsqueeze(0).tile(batch_size, 1, 1)\n",
    "        # weight_hh_batch 的大小应该是 batch_size*h_dim*h_dim\n",
    "        weight_hh_batch = weight_hh.unsqueeze(0).tile(batch_size, 1, 1)\n",
    "        # torch.bmm() 带有batch_size的矩阵相乘\n",
    "        w_times_x = torch.bmm(weight_ih_batch, x.unsqueeze(2))  #此时的x应该是 batch_size*input_size*1\n",
    "        #w_times_x 的最后的结果应该是batch_size*h_dim*1，然后可以squeeze(-1)掉\n",
    "        w_times_x = w_times_x.squeeze(-1) # batch_size*h_dim\n",
    "        \n",
    "        # h_prve最开始是batch_size*hidden_size，现在需要把它扩充到batch_size*hidden_size*1\n",
    "        w_times_h = torch.bmm(weight_hh_batch, h_prev.unsqueeze(2)).squeeze(-1)  # batch_size*h_dim \n",
    "        # 最终的计算并且不断更新\n",
    "        h_prev = torch.tanh(w_times_x + bias_ih + w_times_h + bias_hh)\n",
    "        h_out[:, t, :] = h_prev\n",
    "        \n",
    "    return h_out, h_prev.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a9f96672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_ih_l0 Parameter containing:\n",
      "tensor([[-0.4625,  0.5091,  0.5695, -0.1479,  0.2251, -0.5359],\n",
      "        [ 0.1143, -0.0672, -0.1061, -0.5320, -0.1582, -0.4026],\n",
      "        [ 0.5427, -0.3619, -0.1699,  0.1750,  0.2764, -0.1850]],\n",
      "       requires_grad=True)\n",
      "weight_hh_l0 Parameter containing:\n",
      "tensor([[ 0.1400,  0.3478,  0.3506],\n",
      "        [-0.4183, -0.2382, -0.5081],\n",
      "        [ 0.2111,  0.0751, -0.1017]], requires_grad=True)\n",
      "bias_ih_l0 Parameter containing:\n",
      "tensor([-0.3031,  0.4574,  0.5712], requires_grad=True)\n",
      "bias_hh_l0 Parameter containing:\n",
      "tensor([ 0.3753, -0.0053, -0.3025], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 验证rnn_forward函数的准确性\n",
    "for k,v in rnn.named_parameters():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3fa474e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnn_forward output:\n",
      "tensor([[[ 0.8493, -0.5992, -0.2526],\n",
      "         [-0.6106, -0.1691,  0.5180],\n",
      "         [ 0.6820,  0.9631,  0.0999],\n",
      "         [ 0.0734, -0.5295,  0.9324]]], grad_fn=<CopySlices>)\n",
      "tensor([[[ 0.0734, -0.5295,  0.9324]]], grad_fn=<UnsqueezeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "custom_rnn_output, custom_state_fina = rnn_forward(input, rnn.weight_ih_l0, rnn.bias_ih_l0, rnn.weight_hh_l0, rnn.bias_hh_l0, h_prev)\n",
    "print(\"rnn_forward output:\")\n",
    "print(custom_rnn_output)\n",
    "print(custom_state_fina)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d73070e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bidirectional_rnn_forward 函数手写实现，实现双向RNN计算\n",
    "def bi_rnn_forward(input, weight_ih, bias_ih, weight_hh, bias_hh, h_prev, weight_ih_reverse, weight_hh_reverse, bias_ih_reverse, bias_hh_reverse, h_prev_reverse):\n",
    "    # input的shape对应三个参数：batch_size, sequence_length, input_size\n",
    "    batch_size, sequence_length, input_size = input.shape\n",
    "    # h_dim代表隐藏层的维度，也就是隐藏层的层数 [0]是因为矩阵相乘第一个维度作为不抵消的维度\n",
    "    h_dim = weight_ih.shape[0]  \n",
    "    # 初始化一个状态（输出）矩阵\n",
    "    h_out = torch.zeros(batch_size, sequence_length, h_dim*2) # h_dim就是hidden_size，现在应该是双层结构，所以是两倍\n",
    "    \n",
    "    \n",
    "    # 两层都可以调用我们自己写的rnn_forward() \n",
    "    forward_output = rnn_forward(input, weight_ih, bias_ih, weight_hh, bias_hh, h_prev)[0]  # forward_layer\n",
    "    # backward_layer 的input应该是相较于forward反向过来，具体的反向是input中的sequence（词元）进行反向\n",
    "    backward_output = rnn_forward(torch.flip(input, [1]), weight_ih_reverse, weight_hh_reverse, bias_ih_reverse, bias_hh_reverse, h_prev_reverse)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
