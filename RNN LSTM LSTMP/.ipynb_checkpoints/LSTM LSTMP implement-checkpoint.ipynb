{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0881e82",
   "metadata": {},
   "source": [
    "# LSTM的 API调用实现和 手写LSTM实现\n",
    "\n",
    "class Torch.nn.LSTM()\n",
    "\n",
    "### 公式\n",
    "\n",
    "$$i_{t} = \\sigma (W_{ii}x_{t}+b_{ii}+W_{hi}h_{t-1}+b_{hi})$$\n",
    "$$f_{t} = \\sigma (W_{if}x_{t}+b_{if}+W_{hf}h_{t-1}+b_{hf})$$\n",
    "$$g_{t} = tanh(W_{ig}x_{t}+b_{ig}+W_{hg}h_{t−1}+b_{hg})$$\n",
    "$$o_{t} = \\sigma (W_{io}x_{t}+b_{io}+W_{ho}h{t−1}+b_{ho})$$\n",
    "$$c_{t} = f_{t} \\odot c_{t−1}+i_{t} \\odot g_{t}$$\n",
    "$$h_{t} = o_{t} \\odot tanh(c_{t})$$\n",
    "\n",
    "> 其中直接相乘是矩阵乘法，点乘是hadamard乘积\n",
    "\n",
    "- inputs: input, (h_0, c_0) 两个涉及到更新迭代的变量都需要传入初始值，并且是一个元组的形式\n",
    "- outputs: output, (h_n, c_n)\n",
    "\n",
    "$$N = batch size$$\n",
    "$$L = sequence length$$\n",
    "$$D = 2  \\enspace if \\enspace bidirectional=True \\enspace otherwise  \\enspace 1$$\n",
    "$$H_{in} = input_size$$\n",
    "$$H_{cell} = hidden_size$$\n",
    "$$H_{out} = \\enspace proj_size \\enspace if \\enspace proj_size>0 \\enspace otherwise \\enspace hidden_size$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "226ed3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0954, -0.3666, -0.7522,  0.5803, -0.2007],\n",
      "         [ 0.3062, -0.1573, -0.4875,  0.3546, -0.0033],\n",
      "         [ 0.2897, -0.1801, -0.3915,  0.3521,  0.0440]],\n",
      "\n",
      "        [[ 0.2178,  0.2259, -0.3785,  0.0323, -0.0202],\n",
      "         [-0.0134, -0.0893, -0.1599, -0.0622, -0.2748],\n",
      "         [-0.0743, -0.0655, -0.2645, -0.1283, -0.0643]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "(tensor([[[ 0.2897, -0.1801, -0.3915,  0.3521,  0.0440],\n",
      "         [-0.0743, -0.0655, -0.2645, -0.1283, -0.0643]]],\n",
      "       grad_fn=<StackBackward0>), tensor([[[ 0.6266, -0.6600, -0.7663,  0.4914,  0.1441],\n",
      "         [-0.2508, -0.1600, -0.3482, -0.1871, -0.1418]]],\n",
      "       grad_fn=<StackBackward0>))\n"
     ]
    }
   ],
   "source": [
    "# 实现LSTM和LSTMP的源码\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# 首先定义一些常量\n",
    "batch_size, sequence_length, input_size, hidden_size = 2, 3, 4, 5 \n",
    "# batch_size 样本大小 sequence_length 词元大小 input_size 特征向量大小 hidden_size 细胞大小\n",
    "# proj_size 大小待定\n",
    "# 初始化一个正态分布的初始输入\n",
    "input = torch.randn(batch_size, sequence_length, input_size) # 输入序列\n",
    "\n",
    "# 生成c_0 和 h_0，假设我们只考虑一层LSTM网络\n",
    "c_0 = torch.randn(batch_size, hidden_size) # 不参与训练，就是一个初始值\n",
    "h_0 = torch.randn(batch_size, hidden_size) # 同样不参与训练\n",
    "\n",
    "# 调用官方的API\n",
    "lstm_layout = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "\n",
    "# 传入输入和状态\n",
    "output, (h_n, c_n) = lstm_layout(input, (h_0.unsqueeze(0), c_0.unsqueeze(0))) # 初始化的h0和c0大小都是 N*H_out 所以需要在第一维扩容\n",
    "\n",
    "print(output)\n",
    "print((h_n, c_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11605e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_ih_l0 Parameter containing:\n",
      "tensor([[-0.3608,  0.4223,  0.3240,  0.4396],\n",
      "        [ 0.2628, -0.0082, -0.1540, -0.4255],\n",
      "        [ 0.0675,  0.2024, -0.3961,  0.3381],\n",
      "        [ 0.0352,  0.2174,  0.4350,  0.1006],\n",
      "        [ 0.3904, -0.0922, -0.2534, -0.4359],\n",
      "        [ 0.3478,  0.1939, -0.2691,  0.3185],\n",
      "        [-0.1673,  0.2237, -0.0575, -0.3648],\n",
      "        [-0.2703, -0.3033,  0.0513,  0.2277],\n",
      "        [ 0.2730,  0.0737, -0.0360,  0.2822],\n",
      "        [-0.0256,  0.2518,  0.2062, -0.4149],\n",
      "        [ 0.2545, -0.2313,  0.4291,  0.3674],\n",
      "        [ 0.0853,  0.3815,  0.4442,  0.4279],\n",
      "        [ 0.3721, -0.0452,  0.1054, -0.1018],\n",
      "        [ 0.0199, -0.2720, -0.3964, -0.1091],\n",
      "        [-0.0961, -0.0362, -0.0221,  0.2594],\n",
      "        [ 0.0199, -0.2054,  0.3756, -0.0886],\n",
      "        [-0.4058,  0.0785, -0.0532, -0.0588],\n",
      "        [ 0.3822,  0.3860, -0.4194,  0.1636],\n",
      "        [ 0.3558, -0.2027, -0.0279,  0.3890],\n",
      "        [ 0.2146,  0.3917, -0.1237, -0.3683]], requires_grad=True)\n",
      "weight_hh_l0 Parameter containing:\n",
      "tensor([[-0.2179, -0.1655, -0.2881, -0.3759, -0.3672],\n",
      "        [-0.3950, -0.4422,  0.4265, -0.1898,  0.0766],\n",
      "        [ 0.2914, -0.1473, -0.1166,  0.2275, -0.0052],\n",
      "        [-0.0951,  0.2472, -0.1737,  0.0268, -0.1485],\n",
      "        [-0.3513, -0.0130, -0.0274, -0.0434, -0.3179],\n",
      "        [-0.4052,  0.0588, -0.3094,  0.3042, -0.1424],\n",
      "        [ 0.1459,  0.3633,  0.2132, -0.3245,  0.0925],\n",
      "        [-0.3918, -0.3204,  0.1393,  0.4361,  0.0532],\n",
      "        [-0.0864, -0.2253, -0.2120, -0.3284,  0.1150],\n",
      "        [-0.3802,  0.2317, -0.4262, -0.1282, -0.3260],\n",
      "        [ 0.2003, -0.2243, -0.4330, -0.0199,  0.2954],\n",
      "        [-0.1070, -0.3755,  0.0216, -0.1783, -0.1159],\n",
      "        [-0.2308,  0.1030, -0.1693, -0.3883, -0.3212],\n",
      "        [ 0.3884, -0.2349, -0.0784, -0.2756,  0.3014],\n",
      "        [-0.0428, -0.0806, -0.3537,  0.1997,  0.2035],\n",
      "        [ 0.1163,  0.0611, -0.3174,  0.1270, -0.2593],\n",
      "        [-0.3696,  0.3773,  0.2192,  0.0322,  0.3512],\n",
      "        [ 0.0212,  0.2784, -0.2545,  0.3639,  0.3264],\n",
      "        [-0.0385,  0.2614, -0.4273, -0.0232, -0.4162],\n",
      "        [ 0.3090,  0.2209, -0.4449, -0.0501, -0.0798]], requires_grad=True)\n",
      "bias_ih_l0 Parameter containing:\n",
      "tensor([ 0.4358,  0.2235, -0.0800,  0.1119,  0.2237, -0.0017, -0.2249,  0.4330,\n",
      "         0.0180, -0.2647, -0.3271, -0.0792, -0.0800,  0.2288, -0.2573, -0.2106,\n",
      "        -0.3273,  0.4325,  0.4167, -0.2907], requires_grad=True)\n",
      "bias_hh_l0 Parameter containing:\n",
      "tensor([ 0.4410,  0.2637, -0.0964,  0.3405,  0.2588,  0.1386,  0.1320, -0.2906,\n",
      "         0.0468, -0.3027,  0.4019, -0.2664, -0.3922, -0.3607,  0.2909, -0.2695,\n",
      "         0.1083,  0.2438,  0.2705, -0.3674], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 查看lstm_layout内部的权重和张量名字\n",
    "for k,v in lstm_layout.named_parameters():\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e8ab0e",
   "metadata": {},
   "source": [
    "### 直接看这四个参数对应的shape大小\n",
    "\n",
    "> weight_ih_l0 对应的是和x矩阵相乘的四个参数，把他们合并到了一起来\n",
    "> weight_hh_l0 对应的是和h矩阵相乘的四个参数\n",
    "> 两个bias原理同上，都是合并到了一起来\n",
    "\n",
    "- weight_ih的四个参数，隐藏层的神经元是5，所以四个参数合在一起就是20\n",
    "- 矩阵乘法，与输入向量相乘，权值矩阵的第一维是输出矩阵的特征维数，第二维是输入矩阵的\n",
    "- weight_ih 是要和 input_size相乘的，所以input_size是4\n",
    "- 同理 hh应该和 hidden_size相乘，所以是5(矩阵相乘后一维度)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b727a688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 手写LSTM模型\n",
    "def lstm_forward(input, initial_states, w_ih, w_hh, b_ih, b_hh):\n",
    "    h_0, c_0 = initial_states  # 初始状态的拆解 batch_size*hidden_size\n",
    "    batch_size, sequence_length, input_size = input.shape # 输入的拆解\n",
    "    hidden_size = w_ih.shape[0] // 4 # 第0维除以4就是拆解hidden_size\n",
    "    output_size = hidden_size\n",
    "    \n",
    "    h_prev, c_prev = h_0, c_0 # 每一时刻的h和c在后面要进行迭代\n",
    "    # 初始化output_size\n",
    "    output = torch.zeros(batch_size, sequence_length, output_size) # 初始化输出序列\n",
    "    \n",
    "    # 确定维度 并且提高维度\n",
    "    batch_w_ih = w_ih.unsqueeze(0).tile(batch_size, 1, 1) # batch_size*4*hidden_size*input_size  扩容batch_size\n",
    "    batch_w_hh = w_hh.unsqueeze(0).tile(batch_size, 1, 1) # batch_size*4*hidden_size*hidden_size\n",
    "    \n",
    "    for i in range(sequence_length):\n",
    "        x = input[:, i, :]  # 当前时刻的输入向量 维度 batch_size*input_size\n",
    "        w_times_x = torch.bmm(batch_w_ih, x.unsqueeze(-1)).squeeze(-1)  #维度 batch_size*4*hidden_size*1 然后降维把1删掉\n",
    "        w_times_h = torch.bmm(batch_w_hh, h_prev.unsqueeze(-1)).squeeze(-1)  #维度 batch_size*4*hidden_size*1 然后降维把1删掉\n",
    "        \n",
    "        \n",
    "        # 开始分别计算输入门(i)，遗忘门(f)，cell门(g)，和输出门(o)\n",
    "        i_times_t = torch.sigmoid(w_times_x[:, :, hidden_size] + w_times_h[:, :, hidden_size] + b_ih[:hidden_size], b_hh[:hidden_size])\n",
    "        f_times_t = torch.sigmoid(w_times_x[:, :, hidden_size:2*hidden_size] + w_times_h[:, :, hidden_size:2*hidden_size] + \\\n",
    "                                  b_ih[hidden_size:2*hidden_size], b_hh[hidden_size:2*hidden_size])\n",
    "        g_times_t = torch.tanh(w_times_x[:, :, 2*hidden_size:3*hidden_size] + w_times_h[:, :, 2*hidden_size:3*hidden_size] + \\\n",
    "                                  b_ih[2*hidden_size:3*hidden_size], b_hh[2*hidden_size:3*hidden_size])\n",
    "        o_times_t = torch.sigmoid(w_times_x[:, :, 3*hidden_size:] + w_times_h[:, :, 3*hidden_size:] + \\\n",
    "                                  b_ih[3*hidden_size:], b_hh[3*hidden_size:])\n",
    "        \n",
    "        # 细胞状态c_t/c_prev\n",
    "        c_prev = f_times_t * c_prev + i_times_t * g_times_t\n",
    "        h_prev = o_times_t * torch.tanh(c_prev)\n",
    "        \n",
    "        output[:, i, :] = h_prev\n",
    "        \n",
    "    return output, (h_prev, c_prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ff3422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证正确性，将lstm_layout生成的张量代入进去\n",
    "custom_output, (custom_h, custom_c) = lstm_forward(input, (h_0, c_0), lstm_layout.weight_ih_l0, lstm_layout.weight_hh_l0, lstm_layout.bias_ih_l0, lstm_layout.bias_hh_l0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
