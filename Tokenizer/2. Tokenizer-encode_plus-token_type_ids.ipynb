{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf87f314",
   "metadata": {},
   "source": [
    "## 加载模型 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45db7aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c4cd3a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer  # 语料大小 30522 右侧padding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d013b51d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'unk_token': '[UNK]',\n",
       " 'sep_token': '[SEP]',\n",
       " 'pad_token': '[PAD]',\n",
       " 'cls_token': '[CLS]',\n",
       " 'mask_token': '[MASK]'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ca145d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100, 102, 0, 101, 103]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_tokens_list = list(tokenizer.special_tokens_map.values())\n",
    "tokenizer.convert_tokens_to_ids(special_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e547bfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 100, 102, 0, 101, 103, 102]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(special_tokens_list)  # 开头加了101 结尾加了102"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a63d0f",
   "metadata": {},
   "source": [
    "## 认识文本语料\n",
    "\n",
    "\n",
    "#### fetch_20newsgroups\n",
    "\n",
    "20 newsgroups数据集18000多篇新闻文章，一共涉及到20种话题，所以称作20newsgroups text dataset，分为两部分：训练集和测试集，通常用来做文本分类，均匀分为20个不同主题的新闻组集合。20newsgroups数据集是被用于文本分类、文本挖据和信息检索研究的国际标准数据集之一。一些新闻组的主题特别相似(e.g. comp.sys.ibm.pc.hardware/ comp.sys.mac.hardware)，还有一些却完全不相关 (e.g misc.forsale /soc.religion.christian)。\n",
    "\n",
    "\n",
    "- newsgroups_train.DESCR  关于dataset的基本介绍\n",
    "- newsgroups_train.data  type list length 11314 \n",
    "- newsgroups_train.target 分类  length 11314\n",
    "- newsgroups_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33326b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8eb8714",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55d8a594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({7: 594,\n",
       "         4: 578,\n",
       "         1: 584,\n",
       "         14: 593,\n",
       "         16: 546,\n",
       "         13: 594,\n",
       "         3: 590,\n",
       "         2: 591,\n",
       "         8: 598,\n",
       "         19: 377,\n",
       "         6: 585,\n",
       "         0: 480,\n",
       "         12: 591,\n",
       "         5: 593,\n",
       "         10: 600,\n",
       "         9: 597,\n",
       "         15: 599,\n",
       "         17: 564,\n",
       "         18: 465,\n",
       "         11: 595})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(newsgroups_train.target)  # 一共是有19中分类，每一个分类对应的数量大概是600左右"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb4a2786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91867efe",
   "metadata": {},
   "source": [
    "## Tokenizer 补充\n",
    "\n",
    "- input_ids, attention_mask\n",
    "- encode_plus, token_type_ids\n",
    "    - 有些NLP任务需要将两个句子拼接在一起，比如序列标注/分类和问答。例如问答时，需要第一个作为上下文，第二个句子作为问题，要求模型输出答案。这时tokenizer接受两个句子的顺序输入并输出数字编码。虽然返回的数字编码中也包含了句子的分隔信息，Tokenizer的输出仍然提供可选的第3个常用字段\"token_type_ids\"。它用来表明返回的数字编码中哪些属于第一个句子，哪些属于第二个句子。\n",
    "    - 句子对一般是用在nsp任务中 next sentence predict, bert的预训练任务之一"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2490eac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"From: lerxst@wam.umd.edu (where's my thing)\\nSubject: WHAT car is this!?\\nNntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\n\\n I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\\n\\nThanks,\\n- IL\\n   ---- brought to you by your neighborhood Lerxst ----\\n\\n\\n\\n\\n\",\n",
       " \"From: guykuo@carson.u.washington.edu (Guy Kuo)\\nSubject: SI Clock Poll - Final Call\\nSummary: Final call for SI clock reports\\nKeywords: SI,acceleration,clock,upgrade\\nArticle-I.D.: shelley.1qvfo9INNc3s\\nOrganization: University of Washington\\nLines: 11\\nNNTP-Posting-Host: carson.u.washington.edu\\n\\nA fair number of brave souls who upgraded their SI clock oscillator have\\nshared their experiences for this poll. Please send a brief message detailing\\nyour experiences with the procedure. Top speed attained, CPU rated speed,\\nadd on cards and adapters, heat sinks, hour of usage per day, floppy disk\\nfunctionality with 800 and 1.4 m floppies are especially requested.\\n\\nI will be summarizing in the next two days, so please add to the network\\nknowledge base if you have done the clock upgrade and haven't answered this\\npoll. Thanks.\\n\\nGuy Kuo <guykuo@u.washington.edu>\\n\",\n",
       " 'From: twillis@ec.ecn.purdue.edu (Thomas E Willis)\\nSubject: PB questions...\\nOrganization: Purdue University Engineering Computer Network\\nDistribution: usa\\nLines: 36\\n\\nwell folks, my mac plus finally gave up the ghost this weekend after\\nstarting life as a 512k way back in 1985.  sooo, i\\'m in the market for a\\nnew machine a bit sooner than i intended to be...\\n\\ni\\'m looking into picking up a powerbook 160 or maybe 180 and have a bunch\\nof questions that (hopefully) somebody can answer:\\n\\n* does anybody know any dirt on when the next round of powerbook\\nintroductions are expected?  i\\'d heard the 185c was supposed to make an\\nappearence \"this summer\" but haven\\'t heard anymore on it - and since i\\ndon\\'t have access to macleak, i was wondering if anybody out there had\\nmore info...\\n\\n* has anybody heard rumors about price drops to the powerbook line like the\\nones the duo\\'s just went through recently?\\n\\n* what\\'s the impression of the display on the 180?  i could probably swing\\na 180 if i got the 80Mb disk rather than the 120, but i don\\'t really have\\na feel for how much \"better\" the display is (yea, it looks great in the\\nstore, but is that all \"wow\" or is it really that good?).  could i solicit\\nsome opinions of people who use the 160 and 180 day-to-day on if its worth\\ntaking the disk size and money hit to get the active display?  (i realize\\nthis is a real subjective question, but i\\'ve only played around with the\\nmachines in a computer store breifly and figured the opinions of somebody\\nwho actually uses the machine daily might prove helpful).\\n\\n* how well does hellcats perform?  ;)\\n\\nthanks a bunch in advance for any info - if you could email, i\\'ll post a\\nsummary (news reading time is at a premium with finals just around the\\ncorner... :( )\\n--\\nTom Willis  \\\\  twillis@ecn.purdue.edu    \\\\    Purdue Electrical Engineering\\n---------------------------------------------------------------------------\\n\"Convictions are more dangerous enemies of truth than lies.\"  - F. W.\\nNietzsche\\n']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 筛选出来前三个数据进行demo\n",
    "test_news = newsgroups_train.data[:3]\n",
    "test_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "128fd6d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[721, 858, 1981]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(i) for i in test_news]  # 每一个demo新闻的长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "881d897a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 2013, 1024, 3393, 2099, 2595, 3367, 1030, 11333, 2213, 1012, 8529, 2094, 1012, 3968, 2226, 1006, 2073, 1005, 1055, 2026, 2518, 1007, 3395, 1024, 2054, 2482, 2003, 2023, 999, 1029, 102], [101, 2013, 1024, 3124, 5283, 2080, 1030, 9806, 1012, 1057, 1012, 2899, 1012, 3968, 2226, 1006, 3124, 13970, 2080, 1007, 3395, 1024, 9033, 5119, 8554, 1011, 2345, 2655, 12654, 1024, 2345, 102], [101, 2013, 1024, 1056, 29602, 6856, 1030, 14925, 1012, 14925, 2078, 1012, 19749, 1012, 3968, 2226, 1006, 2726, 1041, 12688, 1007, 3395, 1024, 1052, 2497, 3980, 1012, 1012, 1012, 3029, 1024, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# single sentence\n",
    "tokenizer(test_news, truncation=True, max_length=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b6df29d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded sequence(AB): [101, 2013, 1024, 3393, 2099, 2595, 3367, 1030, 11333, 2213, 1012, 8529, 2094, 1012, 3968, 2226, 1006, 2073, 1005, 1055, 2026, 2518, 1007, 3395, 1024, 2054, 2482, 2003, 2023, 999, 1029, 1050, 3372, 2361, 1011, 14739, 1011, 3677, 1024, 10958, 2278, 2509, 1012, 11333, 2213, 1012, 8529, 2094, 1012, 3968, 2226, 3029, 1024, 2118, 1997, 5374, 1010, 2267, 2380, 3210, 1024, 2321, 1045, 2001, 6603, 2065, 3087, 2041, 2045, 2071, 4372, 7138, 2368, 2033, 2006, 2023, 2482, 1045, 2387, 1996, 2060, 2154, 1012, 2009, 2001, 1037, 1016, 1011, 2341, 2998, 2482, 1010, 2246, 2000, 2022, 2013, 1996, 2397, 20341, 1013, 2220, 17549, 1012, 2009, 2001, 2170, 1037, 5318, 4115, 1012, 1996, 4303, 2020, 2428, 2235, 1012, 1999, 2804, 1010, 1996, 2392, 21519, 2001, 3584, 2013, 1996, 2717, 1997, 1996, 2303, 1012, 2023, 2003, 2035, 1045, 2113, 1012, 2065, 3087, 2064, 2425, 4168, 1037, 2944, 2171, 1010, 3194, 28699, 2015, 1010, 2086, 1997, 2537, 1010, 2073, 2023, 2482, 2003, 2081, 1010, 2381, 1010, 2030, 3649, 18558, 2017, 2031, 2006, 2023, 24151, 2559, 2482, 1010, 3531, 1041, 1011, 5653, 1012, 4283, 1010, 1011, 6335, 1011, 1011, 1011, 1011, 2716, 2000, 2017, 2011, 2115, 5101, 3393, 2099, 2595, 3367, 1011, 1011, 1011, 1011, 102, 2013, 1024, 3124, 5283, 2080, 1030, 9806, 1012, 1057, 1012, 2899, 1012, 3968, 2226, 1006, 3124, 13970, 2080, 1007, 3395, 1024, 9033, 5119, 8554, 1011, 2345, 2655, 12654, 1024, 2345, 2655, 2005, 9033, 5119, 4311, 3145, 22104, 1024, 9033, 1010, 16264, 1010, 5119, 1010, 12200, 3720, 1011, 1045, 1012, 1040, 1012, 1024, 15828, 1012, 1015, 4160, 2615, 14876, 2683, 23111, 2278, 2509, 2015, 3029, 1024, 2118, 1997, 2899, 3210, 1024, 2340, 1050, 3372, 2361, 1011, 14739, 1011, 3677, 1024, 9806, 1012, 1057, 1012, 2899, 1012, 3968, 2226, 1037, 4189, 2193, 1997, 9191, 9293, 2040, 9725, 2037, 9033, 5119, 9808, 6895, 4571, 4263, 2031, 4207, 2037, 6322, 2005, 2023, 8554, 1012, 3531, 4604, 1037, 4766, 4471, 17555, 2115, 6322, 2007, 1996, 7709, 1012, 2327, 3177, 12754, 1010, 17368, 6758, 3177, 1010, 5587, 2006, 5329, 1998, 15581, 2545, 1010, 3684, 23462, 1010, 3178, 1997, 8192, 2566, 2154, 1010, 28491, 9785, 15380, 2007, 5385, 1998, 1015, 1012, 1018, 1049, 28583, 13046, 2024, 2926, 7303, 1012, 1045, 2097, 2022, 7680, 7849, 6026, 1999, 1996, 2279, 2048, 2420, 1010, 2061, 3531, 5587, 2000, 1996, 2897, 3716, 2918, 2065, 2017, 2031, 2589, 1996, 5119, 12200, 1998, 4033, 1005, 1056, 4660, 2023, 8554, 1012, 4283, 1012, 3124, 13970, 2080, 1026, 3124, 5283, 2080, 1030, 1057, 1012, 2899, 1012, 3968, 2226, 1028, 102]\n",
      "Decoded sequence(AB): [CLS] from : lerxst @ wam. umd. edu ( where's my thing ) subject : what car is this!? nntp - posting - host : rac3. wam. umd. edu organization : university of maryland, college park lines : 15 i was wondering if anyone out there could enlighten me on this car i saw the other day. it was a 2 - door sports car, looked to be from the late 60s / early 70s. it was called a bricklin. the doors were really small. in addition, the front bumper was separate from the rest of the body. this is all i know. if anyone can tellme a model name, engine specs, years of production, where this car is made, history, or whatever info you have on this funky looking car, please e - mail. thanks, - il - - - - brought to you by your neighborhood lerxst - - - - [SEP] from : guykuo @ carson. u. washington. edu ( guy kuo ) subject : si clock poll - final call summary : final call for si clock reports keywords : si, acceleration, clock, upgrade article - i. d. : shelley. 1qvfo9innc3s organization : university of washington lines : 11 nntp - posting - host : carson. u. washington. edu a fair number of brave souls who upgraded their si clock oscillator have shared their experiences for this poll. please send a brief message detailing your experiences with the procedure. top speed attained, cpu rated speed, add on cards and adapters, heat sinks, hour of usage per day, floppy disk functionality with 800 and 1. 4 m floppies are especially requested. i will be summarizing in the next two days, so please add to the network knowledge base if you have done the clock upgrade and haven't answered this poll. thanks. guy kuo < guykuo @ u. washington. edu > [SEP]\n",
      "Token type ids(AB): [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# 如果涉及到句子拼接,那么token_type_ids就不全为0，第一个句子是0，第二个句子是1\n",
    "\n",
    "encodings_ab = tokenizer.encode_plus(text=test_news[0], text_pair=test_news[1])  # 目前只能是两个句子的拼接\n",
    "print(\"Encoded sequence(AB):\", encodings_ab[\"input_ids\"])\n",
    "\n",
    "decoded_ab = tokenizer.decode(encodings_ab[\"input_ids\"])\n",
    "print(\"Decoded sequence(AB):\", decoded_ab)\n",
    "print(\"Token type ids(AB):\", encodings_ab[\"token_type_ids\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
