{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "418e7368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "import re\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "from random import *\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3171db4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拟造的一段对话\n",
    "text = (\n",
    "    'Hello, how are you? I am Romeo.\\n' # R\n",
    "    'Hello, Romeo My name is Juliet. Nice to meet you.\\n' # J\n",
    "    'Nice meet you too. How are you today?\\n' # R\n",
    "    'Great. My baseball team won the competition.\\n' # J\n",
    "    'Oh Congratulations, Juliet\\n' # R\n",
    "    'Thank you Romeo\\n' # J\n",
    "    'Where are you going today?\\n' # R\n",
    "    'I am going shopping. What about you?\\n' # J\n",
    "    'I am going to visit my grandmother. she is not very well' # R\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a1e1c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello how are you i am romeo', 'hello romeo my name is juliet nice to meet you', 'nice meet you too how are you today', 'great my baseball team won the competition', 'oh congratulations juliet', 'thank you romeo', 'where are you going today', 'i am going shopping what about you', 'i am going to visit my grandmother she is not very well']\n",
      "\n",
      "wordlist: \n",
      "['romeo', 'the', 'not', 'very', 'too', 'meet', 'is', 'are', 'what', 'juliet', 'i', 'am', 'oh', 'hello', 'competition', 'how', 'well', 'thank', 'great', 'grandmother', 'today', 'to', 'shopping', 'going', 'you', 'visit', 'nice', 'name', 'baseball', 'my', 'she', 'won', 'congratulations', 'where', 'team', 'about']\n",
      "\n",
      "updated word2dix: \n",
      "{'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3, 'romeo': 4, 'the': 5, 'not': 6, 'very': 7, 'too': 8, 'meet': 9, 'is': 10, 'are': 11, 'what': 12, 'juliet': 13, 'i': 14, 'am': 15, 'oh': 16, 'hello': 17, 'competition': 18, 'how': 19, 'well': 20, 'thank': 21, 'great': 22, 'grandmother': 23, 'today': 24, 'to': 25, 'shopping': 26, 'going': 27, 'you': 28, 'visit': 29, 'nice': 30, 'name': 31, 'baseball': 32, 'my': 33, 'she': 34, 'won': 35, 'congratulations': 36, 'where': 37, 'team': 38, 'about': 39}\n",
      "\n",
      "tokenlist:\n",
      "[[17, 19, 11, 28, 14, 15, 4], [17, 4, 33, 31, 10, 13, 30, 25, 9, 28], [30, 9, 28, 8, 19, 11, 28, 24], [22, 33, 32, 38, 35, 5, 18], [16, 36, 13], [21, 28, 4], [37, 11, 28, 27, 24], [14, 15, 27, 26, 12, 39, 28], [14, 15, 27, 25, 29, 33, 23, 34, 10, 6, 7, 20]]\n"
     ]
    }
   ],
   "source": [
    "sentences = re.sub(\"[.,!?\\\\-]\", '', text.lower()).split('\\n') # filter '.', ',', '?', '!'\n",
    "print(sentences)\n",
    "# set 去重， join联合， list转列表\n",
    "word_list = list(set(\" \".join(sentences).split())) # ['hello', 'how', 'are', 'you',...]\n",
    "print(\"\\nwordlist: \")\n",
    "print(word_list)\n",
    "word2idx = {'[PAD]' : 0, '[CLS]' : 1, '[SEP]' : 2, '[MASK]' : 3}\n",
    "# 为每一个词元token创建编号\n",
    "for i, w in enumerate(word_list):\n",
    "    word2idx[w] = i + 4\n",
    "print(\"\\nupdated word2dix: \")\n",
    "print(word2idx)\n",
    "idx2word = {i: w for i, w in enumerate(word2idx)}\n",
    "vocab_size = len(word2idx)\n",
    "\n",
    "token_list = list()\n",
    "for sentence in sentences:\n",
    "    arr = [word2idx[s] for s in sentence.split()]\n",
    "    token_list.append(arr)\n",
    "\n",
    "print(\"\\ntokenlist:\")\n",
    "print(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5db577a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT Parameters\n",
    "maxlen = 30  # 表示同一个 batch 中的所有句子都由 30 个 token 组成，不够的补 PAD\n",
    "batch_size = 6  # \n",
    "max_pred = 5 # max tokens of prediction 表示Bert最多需要预测多少个单词\n",
    "n_layers = 6  \n",
    "n_heads = 12\n",
    "d_model = 768  # 特征维度\n",
    "d_ff = 768*4 # 4*d_model, FeedForward dimension\n",
    "d_k = d_v = 64  # dimension of K(=Q), V\n",
    "n_segments = 2  # 表示 Decoder input 由几句话组成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c072a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample IsNext and NotNext to be same in small batch size\n",
    "def make_data():\n",
    "    batch = []\n",
    "    # pos表示Bert任务中上下文两句话是否相邻，如果是那么pos+1，否则neg+1\n",
    "    positive = negative = 0 \n",
    "    while positive != batch_size/2 or negative != batch_size/2: # 最后争取保证数量是1比1 while循环\n",
    "        # 抽取的是随机某一句话的索引，然后将两句话拼成一句话\n",
    "        tokens_a_index, tokens_b_index = randrange(len(sentences)), randrange(len(sentences)) # sample random index in sentences\n",
    "        tokens_a, tokens_b = token_list[tokens_a_index], token_list[tokens_b_index]\n",
    "        \n",
    "        # 拼接起来 加 cls和 sep\n",
    "        input_ids = [word2idx['[CLS]']] + tokens_a + [word2idx['[SEP]']] + tokens_b + [word2idx['[SEP]']]\n",
    "        segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
    "\n",
    "        # MASK LM  表示一共要做多少个mask\n",
    "        n_pred =  min(max_pred, max(1, int(len(input_ids) * 0.15))) # 15 % of tokens in one sentence\n",
    "        \n",
    "        # 记录 所有不是 special token的位置，也就是说记录原句子中单词的位置索引的list\n",
    "        cand_maked_pos = [i for i, token in enumerate(input_ids)\n",
    "                          if token != word2idx['[CLS]'] and token != word2idx['[SEP]']] # candidate masked position\n",
    "       \n",
    "        shuffle(cand_maked_pos)  # shuffle进行随机排序\n",
    "        # 也就是说可以去做mask的列表我进行随机打乱\n",
    "        \n",
    "        # 存储所有的mask的token词元 和索引\n",
    "        masked_tokens, masked_pos = [], []\n",
    "        \n",
    "        # 已经随机过了，所以取前mask数量的坐标缩进就可以了\n",
    "        for pos in cand_maked_pos[:n_pred]:\n",
    "            masked_pos.append(pos)\n",
    "            masked_tokens.append(input_ids[pos])\n",
    "            \n",
    "            # 80%的概率变成mask， 10%的概率变成另一个单词\n",
    "            if random() < 0.8:  # 80%\n",
    "                input_ids[pos] = word2idx['[MASK]'] # make mask\n",
    "            elif random() > 0.9:  # 10%\n",
    "                index = randint(0, vocab_size - 1) # random index in vocabulary\n",
    "                \n",
    "                # 不能随意替换特殊的 token_map\n",
    "                while index < 4: # can't involve 'CLS', 'SEP', 'PAD'\n",
    "                    index = randint(0, vocab_size - 1)\n",
    "                input_ids[pos] = index # replace\n",
    "\n",
    "        # Zero Paddings 如果这句话不满足maxlen 那么需要padding\n",
    "        n_pad = maxlen - len(input_ids)\n",
    "        input_ids.extend([0] * n_pad)\n",
    "        segment_ids.extend([0] * n_pad)\n",
    "\n",
    "        # Zero Padding (100% - 15%) tokens\n",
    "        if max_pred > n_pred:\n",
    "            n_pad = max_pred - n_pred\n",
    "            masked_tokens.extend([0] * n_pad)\n",
    "            masked_pos.extend([0] * n_pad)\n",
    "\n",
    "        # 这一段主要是判断ab两句话是否为前后衔接的语言，并且给出true和false\n",
    "        if tokens_a_index + 1 == tokens_b_index and positive < batch_size/2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True]) # IsNext\n",
    "            positive += 1\n",
    "        elif tokens_a_index + 1 != tokens_b_index and negative < batch_size/2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False]) # NotNext\n",
    "            negative += 1\n",
    "    return batch\n",
    "# Proprecessing Finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ecabea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1, 14, 15, 27, 25, 29, 33, 23, 34, 10, 6, 7, 20, 2, 3, 15, 27, 26, 12, 39, 28, 2, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [33, 14, 26, 0, 0], [6, 14, 17, 0, 0], False], [[1, 17, 3, 11, 28, 14, 15, 4, 2, 17, 19, 11, 28, 14, 15, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [19, 19, 0, 0, 0], [10, 2, 0, 0, 0], False], [[1, 21, 28, 4, 2, 14, 15, 27, 26, 12, 39, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [28, 0, 0, 0, 0], [11, 0, 0, 0, 0], False], [[1, 30, 9, 28, 8, 19, 11, 28, 3, 2, 22, 33, 32, 38, 35, 3, 18, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [24, 5, 0, 0, 0], [8, 15, 0, 0, 0], True], [[1, 37, 11, 28, 27, 24, 2, 14, 3, 27, 26, 12, 3, 28, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [15, 39, 0, 0, 0], [8, 12, 0, 0, 0], True], [[1, 17, 19, 11, 28, 14, 15, 4, 2, 17, 4, 3, 31, 10, 13, 30, 3, 9, 28, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [10, 33, 25, 0, 0], [13, 11, 16, 0, 0], True]]\n",
      "\n",
      "\n",
      "([1, 14, 15, 27, 25, 29, 33, 23, 34, 10, 6, 7, 20, 2, 3, 15, 27, 26, 12, 39, 28, 2, 0, 0, 0, 0, 0, 0, 0, 0], [1, 17, 3, 11, 28, 14, 15, 4, 2, 17, 19, 11, 28, 14, 15, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 21, 28, 4, 2, 14, 15, 27, 26, 12, 39, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 30, 9, 28, 8, 19, 11, 28, 3, 2, 22, 33, 32, 38, 35, 3, 18, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 37, 11, 28, 27, 24, 2, 14, 3, 27, 26, 12, 3, 28, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 17, 19, 11, 28, 14, 15, 4, 2, 17, 4, 3, 31, 10, 13, 30, 3, 9, 28, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "([33, 14, 26, 0, 0], [19, 19, 0, 0, 0], [28, 0, 0, 0, 0], [24, 5, 0, 0, 0], [15, 39, 0, 0, 0], [10, 33, 25, 0, 0])\n",
      "([6, 14, 17, 0, 0], [10, 2, 0, 0, 0], [11, 0, 0, 0, 0], [8, 15, 0, 0, 0], [8, 12, 0, 0, 0], [13, 11, 16, 0, 0])\n",
      "(False, False, False, True, True, True)\n",
      "\n",
      "\n",
      "tensor([[ 1, 14, 15, 27, 25, 29, 33, 23, 34, 10,  6,  7, 20,  2,  3, 15, 27, 26,\n",
      "         12, 39, 28,  2,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 1, 17,  3, 11, 28, 14, 15,  4,  2, 17, 19, 11, 28, 14, 15,  4,  2,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 1, 21, 28,  4,  2, 14, 15, 27, 26, 12, 39,  3,  2,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 1, 30,  9, 28,  8, 19, 11, 28,  3,  2, 22, 33, 32, 38, 35,  3, 18,  2,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 1, 37, 11, 28, 27, 24,  2, 14,  3, 27, 26, 12,  3, 28,  2,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 1, 17, 19, 11, 28, 14, 15,  4,  2, 17,  4,  3, 31, 10, 13, 30,  3,  9,\n",
      "         28,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0]])\n",
      "tensor([[33, 14, 26,  0,  0],\n",
      "        [19, 19,  0,  0,  0],\n",
      "        [28,  0,  0,  0,  0],\n",
      "        [24,  5,  0,  0,  0],\n",
      "        [15, 39,  0,  0,  0],\n",
      "        [10, 33, 25,  0,  0]])\n",
      "tensor([[ 6, 14, 17,  0,  0],\n",
      "        [10,  2,  0,  0,  0],\n",
      "        [11,  0,  0,  0,  0],\n",
      "        [ 8, 15,  0,  0,  0],\n",
      "        [ 8, 12,  0,  0,  0],\n",
      "        [13, 11, 16,  0,  0]])\n",
      "tensor([0, 0, 0, 1, 1, 1])\n",
      "\n",
      "\n",
      "tensor([[ 1, 14, 15, 27, 25, 29, 33, 23, 34, 10,  6,  7, 20,  2,  3, 15, 27, 26,\n",
      "         12, 39, 28,  2,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 1, 17,  3, 11, 28, 14, 15,  4,  2, 17, 19, 11, 28, 14, 15,  4,  2,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 1, 21, 28,  4,  2, 14, 15, 27, 26, 12, 39,  3,  2,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 1, 30,  9, 28,  8, 19, 11, 28,  3,  2, 22, 33, 32, 38, 35,  3, 18,  2,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 1, 37, 11, 28, 27, 24,  2, 14,  3, 27, 26, 12,  3, 28,  2,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 1, 17, 19, 11, 28, 14, 15,  4,  2, 17,  4,  3, 31, 10, 13, 30,  3,  9,\n",
      "         28,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n"
     ]
    }
   ],
   "source": [
    "batch = make_data()\n",
    "print(batch)\n",
    "\n",
    "\n",
    "# 返回二维矩阵模式，类似于解压包\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = zip(*batch)\n",
    "print(\"\\n\")\n",
    "print(input_ids)\n",
    "print(segment_ids)\n",
    "print(masked_tokens)\n",
    "print(masked_pos)\n",
    "print(isNext)\n",
    "\n",
    "\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = \\\n",
    "    torch.LongTensor(input_ids),  torch.LongTensor(segment_ids), torch.LongTensor(masked_tokens),\\\n",
    "    torch.LongTensor(masked_pos), torch.LongTensor(isNext)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(input_ids)\n",
    "print(segment_ids)\n",
    "print(masked_tokens)\n",
    "print(masked_pos)\n",
    "print(isNext)\n",
    "\n",
    "\n",
    "# 创建自己的dataset一定要实现三个方法 init len 和 getitem\n",
    "\n",
    "class MyDataSet(Data.Dataset):\n",
    "      def __init__(self, input_ids, segment_ids, masked_tokens, masked_pos, isNext):\n",
    "        self.input_ids = input_ids\n",
    "        self.segment_ids = segment_ids\n",
    "        self.masked_tokens = masked_tokens\n",
    "        self.masked_pos = masked_pos\n",
    "        self.isNext = isNext\n",
    "  \n",
    "      def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "  \n",
    "      def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.segment_ids[idx], self.masked_tokens[idx], self.masked_pos[idx], self.isNext[idx]\n",
    "\n",
    "    \n",
    "# 构建dataloader\n",
    "loader = Data.DataLoader(MyDataSet(input_ids, segment_ids, masked_tokens, masked_pos, isNext), batch_size, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8f309bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attn_pad_mask(seq_q, seq_k):\n",
    "    batch_size, seq_len = seq_q.size()\n",
    "    # eq(zero) is PAD token\n",
    "    pad_attn_mask = seq_q.data.eq(0).unsqueeze(1)  # [batch_size, 1, seq_len]\n",
    "    return pad_attn_mask.expand(batch_size, seq_len, seq_len)  # [batch_size, seq_len, seq_len]\n",
    "\n",
    "\n",
    "# GELU 这个激活函数是由 BERT 这篇论文提出来的\n",
    "def gelu(x):\n",
    "    \"\"\"\n",
    "      Implementation of the gelu activation function.\n",
    "      For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n",
    "      0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "      Also see https://arxiv.org/abs/1606.08415\n",
    "    \"\"\"\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.tok_embed = nn.Embedding(vocab_size, d_model)  # token embedding\n",
    "        self.pos_embed = nn.Embedding(maxlen, d_model)  # position embedding\n",
    "        self.seg_embed = nn.Embedding(n_segments, d_model)  # segment(token type) embedding\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, seg):\n",
    "        seq_len = x.size(1)\n",
    "        pos = torch.arange(seq_len, dtype=torch.long)\n",
    "        pos = pos.unsqueeze(0).expand_as(x)  # [seq_len] -> [batch_size, seq_len]\n",
    "        embedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\n",
    "        return self.norm(embedding)\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k) # scores : [batch_size, n_heads, seq_len, seq_len]\n",
    "        scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is one.\n",
    "        attn = nn.Softmax(dim=-1)(scores)\n",
    "        context = torch.matmul(attn, V)\n",
    "        return context\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_V = nn.Linear(d_model, d_v * n_heads)\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        # q: [batch_size, seq_len, d_model], k: [batch_size, seq_len, d_model], v: [batch_size, seq_len, d_model]\n",
    "        residual, batch_size = Q, Q.size(0)\n",
    "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # q_s: [batch_size, n_heads, seq_len, d_k]\n",
    "        k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # k_s: [batch_size, n_heads, seq_len, d_k]\n",
    "        v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # v_s: [batch_size, n_heads, seq_len, d_v]\n",
    "\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1) # attn_mask : [batch_size, n_heads, seq_len, seq_len]\n",
    "\n",
    "        # context: [batch_size, n_heads, seq_len, d_v], attn: [batch_size, n_heads, seq_len, seq_len]\n",
    "        context = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v) # context: [batch_size, seq_len, n_heads * d_v]\n",
    "        output = nn.Linear(n_heads * d_v, d_model)(context)\n",
    "        return nn.LayerNorm(d_model)(output + residual) # output: [batch_size, seq_len, d_model]\n",
    "\n",
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch_size, seq_len, d_model) -> (batch_size, seq_len, d_ff) -> (batch_size, seq_len, d_model)\n",
    "        return self.fc2(gelu(self.fc1(x)))\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention()\n",
    "        self.pos_ffn = PoswiseFeedForwardNet()\n",
    "\n",
    "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
    "        enc_outputs = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) # enc_inputs to same Q,K,V\n",
    "        enc_outputs = self.pos_ffn(enc_outputs) # enc_outputs: [batch_size, seq_len, d_model]\n",
    "        return enc_outputs\n",
    "\n",
    "class BERT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERT, self).__init__()\n",
    "        self.embedding = Embedding()\n",
    "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.classifier = nn.Linear(d_model, 2)\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.activ2 = gelu\n",
    "        # fc2 is shared with embedding layer\n",
    "        embed_weight = self.embedding.tok_embed.weight\n",
    "        self.fc2 = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        self.fc2.weight = embed_weight\n",
    "\n",
    "    def forward(self, input_ids, segment_ids, masked_pos):\n",
    "        output = self.embedding(input_ids, segment_ids) # [bach_size, seq_len, d_model]\n",
    "        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids) # [batch_size, maxlen, maxlen]\n",
    "        for layer in self.layers:\n",
    "            # output: [batch_size, max_len, d_model]\n",
    "            output = layer(output, enc_self_attn_mask)\n",
    "        # it will be decided by first token(CLS)\n",
    "        h_pooled = self.fc(output[:, 0]) # [batch_size, d_model]\n",
    "        logits_clsf = self.classifier(h_pooled) # [batch_size, 2] predict isNext\n",
    "\n",
    "        masked_pos = masked_pos[:, :, None].expand(-1, -1, d_model) # [batch_size, max_pred, d_model]\n",
    "        h_masked = torch.gather(output, 1, masked_pos) # masking position [batch_size, max_pred, d_model]\n",
    "        h_masked = self.activ2(self.linear(h_masked)) # [batch_size, max_pred, d_model]\n",
    "        logits_lm = self.fc2(h_masked) # [batch_size, max_pred, vocab_size]\n",
    "        return logits_lm, logits_clsf\n",
    "model = BERT()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bd3a6f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0010 loss = 1.540012\n",
      "Epoch: 0020 loss = 0.956202\n",
      "Epoch: 0030 loss = 0.845032\n",
      "Epoch: 0040 loss = 0.777330\n",
      "Epoch: 0050 loss = 0.785583\n",
      "Epoch: 0060 loss = 0.797668\n",
      "Epoch: 0070 loss = 0.753545\n",
      "Epoch: 0080 loss = 0.763203\n",
      "Epoch: 0090 loss = 0.743921\n",
      "Epoch: 0100 loss = 0.757495\n",
      "Epoch: 0110 loss = 0.808400\n",
      "Epoch: 0120 loss = 0.813204\n",
      "Epoch: 0130 loss = 0.717861\n",
      "Epoch: 0140 loss = 0.772429\n",
      "Epoch: 0150 loss = 0.755449\n",
      "Epoch: 0160 loss = 0.750143\n",
      "Epoch: 0170 loss = 0.783872\n",
      "Epoch: 0180 loss = 0.759006\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(180):\n",
    "    for input_ids, segment_ids, masked_tokens, masked_pos, isNext in loader:\n",
    "        logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos)\n",
    "        loss_lm = criterion(logits_lm.view(-1, vocab_size), masked_tokens.view(-1)) # for masked LM\n",
    "        loss_lm = (loss_lm.float()).mean()\n",
    "        loss_clsf = criterion(logits_clsf, isNext) # for sentence classification\n",
    "        loss = loss_lm + loss_clsf\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print('Epoch:', '%04d' % (epoch + 1), 'loss =', '{:.6f}'.format(loss))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "57e04c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, how are you? I am Romeo.\n",
      "Hello, Romeo My name is Juliet. Nice to meet you.\n",
      "Nice meet you too. How are you today?\n",
      "Great. My baseball team won the competition.\n",
      "Oh Congratulations, Juliet\n",
      "Thank you Romeo\n",
      "Where are you going today?\n",
      "I am going shopping. What about you?\n",
      "I am going to visit my grandmother. she is not very well\n",
      "['[CLS]', 'i', 'am', 'going', 'to', 'visit', 'my', 'grandmother', 'she', 'is', 'not', 'very', 'well', '[SEP]', '[MASK]', 'am', 'going', 'shopping', 'what', 'about', 'you', '[SEP]']\n",
      "masked tokens list :  [33, 14, 26]\n",
      "predict masked tokens list :  [33, 14, 26]\n",
      "isNext :  False\n",
      "predict isNext :  True\n"
     ]
    }
   ],
   "source": [
    "# Predict mask tokens ans isNext\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = batch[0]\n",
    "print(text)\n",
    "print([idx2word[w] for w in input_ids if idx2word[w] != '[PAD]'])\n",
    "\n",
    "logits_lm, logits_clsf = model(torch.LongTensor([input_ids]), \\\n",
    "                 torch.LongTensor([segment_ids]), torch.LongTensor([masked_pos]))\n",
    "logits_lm = logits_lm.data.max(2)[1][0].data.numpy()\n",
    "print('masked tokens list : ',[pos for pos in masked_tokens if pos != 0])\n",
    "print('predict masked tokens list : ',[pos for pos in logits_lm if pos != 0])\n",
    "\n",
    "logits_clsf = logits_clsf.data.max(1)[1].data.numpy()[0]\n",
    "print('isNext : ', True if isNext else False)\n",
    "print('predict isNext : ',True if logits_clsf else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8591d0ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
