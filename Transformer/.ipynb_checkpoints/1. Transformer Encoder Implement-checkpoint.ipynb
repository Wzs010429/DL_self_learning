{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7a9008c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1585, -0.0324, -0.0425, -0.0417, -0.0419],\n",
      "        [-0.0324,  0.1371, -0.0353, -0.0346, -0.0348],\n",
      "        [-0.0425, -0.0353,  0.1689, -0.0455, -0.0457],\n",
      "        [-0.0417, -0.0346, -0.0455,  0.1666, -0.0448],\n",
      "        [-0.0419, -0.0348, -0.0457, -0.0448,  0.1671]])\n",
      "tensor([[ 1.2860e-04, -1.4124e-16, -9.3983e-05, -1.3631e-05, -2.0983e-05],\n",
      "        [-1.4124e-16,  1.0981e-12, -8.0246e-13, -1.1639e-13, -1.7916e-13],\n",
      "        [-9.3983e-05, -8.0246e-13,  1.9676e-01, -7.7448e-02, -1.1922e-01],\n",
      "        [-1.3631e-05, -1.1639e-13, -7.7448e-02,  9.4752e-02, -1.7291e-02],\n",
      "        [-2.0983e-05, -1.7916e-13, -1.1922e-01, -1.7291e-02,  1.3653e-01]])\n",
      "tensor([0.1975, 0.1640, 0.2153, 0.2112, 0.2121])\n",
      "tensor([1.2861e-04, 1.0981e-12, 7.3074e-01, 1.0599e-01, 1.6314e-01])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11191\\AppData\\Local\\Temp\\ipykernel_19360\\2563856087.py:93: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(score)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "# 关于 word-embedding，以序列建模为例\n",
    "# 考虑 source-sentence 和 target-sentence （暂时考虑离散结果）\n",
    "# 首先构建序列，序列的字符以其在词表中的索引的形式表示出来，首先构建source序列和target序列\n",
    "\n",
    "# seed = 666\n",
    "# torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "# 原序列目标单词的最大原子数（单词表大小）\n",
    "max_num_src_words = 8\n",
    "max_num_tgt_words = 8\n",
    "\n",
    "\n",
    "# 定义特征大小 原API中是512\n",
    "model_dim = 8\n",
    "\n",
    "\n",
    "# 定义单词表的最大序列长度\n",
    "max_src_seq_len = 5\n",
    "max_tgt_seq_len = 5\n",
    "max_position_len = 5\n",
    "\n",
    "batch_size = 4 # 原序列的大小\n",
    "# src_len = torch.randint(2, 5, (batch_size, ))  # 参数：最小值，最大值，数据类型（数据格式）\n",
    "# tgt_len = torch.randint(2, 5, (batch_size, ))\n",
    "src_len = torch.Tensor([2, 4]).to(torch.int32)  # 我们拟造的原序列的长度\n",
    "tgt_len = torch.Tensor([4, 3]).to(torch.int32)  # 定义的目标序列长度\n",
    "\n",
    "# print(src_len)  # tensor([2, 4], dtype=torch.int32)\n",
    "# print(tgt_len)  # tensor([4, 3], dtype=torch.int32)\n",
    "\n",
    "\n",
    "# 生成原序列 这是以单词索引构成的句子 构建batch\n",
    "src_seq = torch.cat([torch.unsqueeze(F.pad(torch.randint(1, max_num_src_words, (L,)), (0, max_src_seq_len - L)), 0) for L in src_len]) \n",
    "# [tensor([3, 3, 0, 0, 0]), tensor([2, 2, 7, 3, 0])]  这只是第一步我们要做的，然后就是将两个独立的tensor合并成一个tensor，使用到了cat\n",
    "# 合并，然后对于每一个Tensor我们在第0维升维  tensor([[3, 5, 0, 0, 0], [3, 6, 4, 1, 0]])\n",
    "\n",
    "\n",
    "# 生成目标序列 并且按照原序列的形式进行padding 默认值是0\n",
    "tgt_seq = torch.cat([torch.unsqueeze(F.pad(torch.randint(1, max_num_tgt_words, (L,)), (0, max_tgt_seq_len - L)), 0) for L in tgt_len])  \n",
    "# tensor([[1, 6, 1, 4, 0], [1, 5, 4, 0, 0]])\n",
    "\n",
    "# print(src_seq)\n",
    "# print(tgt_seq)\n",
    "\n",
    "\n",
    "# 构造embedding （source embedding 和target embedding）\n",
    "# 在这里+1的含义是我们padding了0，所以要预留出0的dim\n",
    "src_embedding_table = nn.Embedding(max_num_src_words +1, model_dim)\n",
    "tgt_embedding_table = nn.Embedding(max_num_tgt_words +1, model_dim)\n",
    "\n",
    "src_embedding = src_embedding_table(src_seq)\n",
    "tgt_embedding = tgt_embedding_table(tgt_seq)\n",
    "\n",
    "\n",
    "# 构造position embedding\n",
    "position_matrix = torch.arange(max_position_len).reshape((-1, 1))  # PE 两大参数\n",
    "i_matrix = torch.pow(10000, torch.arange(0, model_dim, 2).reshape((1, -1))/model_dim)\n",
    "\n",
    "pe_embedding_table = torch.zeros(max_position_len, model_dim)\n",
    "pe_embedding_table[:, ::2] = torch.sin(position_matrix / i_matrix)  # PE公式中偶数列sin的构建\n",
    "pe_embedding_table[:, 1::2] = torch.cos(position_matrix / i_matrix)  # # PE公式中奇数列cos的构建\n",
    "\n",
    "# 可以利用nn.enbedding来实现快速构建效果\n",
    "pe_embedding = nn.Embedding(max_position_len, model_dim)\n",
    "pe_embedding.weight = nn.Parameter(pe_embedding_table, requires_grad=False)\n",
    "# print(pe_embedding_table)  # 结果应该是和上面一致的\n",
    "\n",
    "src_position = torch.cat([torch.unsqueeze(torch.arange(max(src_len)), 0) for _ in src_len]).to(torch.int32)\n",
    "tgt_position = torch.cat([torch.unsqueeze(torch.arange(max(tgt_len)), 0) for _ in tgt_len]).to(torch.int32)\n",
    "\n",
    "# print(src_position)\n",
    "src_pe_embedding = pe_embedding(src_position)\n",
    "tgt_pe_embedding = pe_embedding(tgt_position)\n",
    "\n",
    "# print(src_pe_embedding)  # 这结果的size是2*4*8 意味着batch_size是2 sequence_length是4 dim是8\n",
    "# print(tgt_pe_embedding)\n",
    "\n",
    "# softmax demo\n",
    "# 首先随机生成一个正态分布来观察分数\n",
    "alpha1 = 0.1\n",
    "alpha2 = 10  # fine-tune 的可能影响\n",
    "score = torch.randn(5) # 假设是Attention(Q, K, V)中QK点集的结果\n",
    "prob1 = F.softmax(score*alpha1, -1)\n",
    "prob2 = F.softmax(score*alpha2, -1)\n",
    "def soft_max(score):\n",
    "    return F.softmax(score)\n",
    "# 查看雅可比矩阵\n",
    "jacobian1 = torch.autograd.functional.jacobian(soft_max, score*alpha1)\n",
    "jacobian2 = torch.autograd.functional.jacobian(soft_max, score*alpha2)\n",
    "print(jacobian1)\n",
    "print(jacobian2)\n",
    "print(prob1)  # tensor([0.1489, 0.1668, 0.0710, 0.4117, 0.2015]) Prob越大代表着两个单词之间的相似性越大\n",
    "print(prob2)  # tensor([4.7223e-08, 2.4097e-08, 3.3442e-04, 4.3119e-05, 9.9962e-01]) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
