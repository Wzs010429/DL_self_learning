{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "418e7368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "import re\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "from random import *\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3171db4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拟造的一段对话\n",
    "text = (\n",
    "    'Hello, how are you? I am Romeo.\\n' # R\n",
    "    'Hello, Romeo My name is Juliet. Nice to meet you.\\n' # J\n",
    "    'Nice meet you too. How are you today?\\n' # R\n",
    "    'Great. My baseball team won the competition.\\n' # J\n",
    "    'Oh Congratulations, Juliet\\n' # R\n",
    "    'Thank you Romeo\\n' # J\n",
    "    'Where are you going today?\\n' # R\n",
    "    'I am going shopping. What about you?\\n' # J\n",
    "    'I am going to visit my grandmother. she is not very well' # R\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a1e1c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello how are you i am romeo', 'hello romeo my name is juliet nice to meet you', 'nice meet you too how are you today', 'great my baseball team won the competition', 'oh congratulations juliet', 'thank you romeo', 'where are you going today', 'i am going shopping what about you', 'i am going to visit my grandmother she is not very well']\n",
      "\n",
      "wordlist: \n",
      "['won', 'hello', 'am', 'very', 'i', 'going', 'where', 'well', 'she', 'is', 'juliet', 'oh', 'congratulations', 'competition', 'meet', 'romeo', 'shopping', 'are', 'name', 'grandmother', 'today', 'about', 'too', 'you', 'the', 'to', 'not', 'visit', 'my', 'baseball', 'great', 'team', 'what', 'thank', 'nice', 'how']\n",
      "\n",
      "updated word2dix: \n",
      "{'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3, 'won': 4, 'hello': 5, 'am': 6, 'very': 7, 'i': 8, 'going': 9, 'where': 10, 'well': 11, 'she': 12, 'is': 13, 'juliet': 14, 'oh': 15, 'congratulations': 16, 'competition': 17, 'meet': 18, 'romeo': 19, 'shopping': 20, 'are': 21, 'name': 22, 'grandmother': 23, 'today': 24, 'about': 25, 'too': 26, 'you': 27, 'the': 28, 'to': 29, 'not': 30, 'visit': 31, 'my': 32, 'baseball': 33, 'great': 34, 'team': 35, 'what': 36, 'thank': 37, 'nice': 38, 'how': 39}\n",
      "\n",
      "tokenlist:\n",
      "[[5, 39, 21, 27, 8, 6, 19], [5, 19, 32, 22, 13, 14, 38, 29, 18, 27], [38, 18, 27, 26, 39, 21, 27, 24], [34, 32, 33, 35, 4, 28, 17], [15, 16, 14], [37, 27, 19], [10, 21, 27, 9, 24], [8, 6, 9, 20, 36, 25, 27], [8, 6, 9, 29, 31, 32, 23, 12, 13, 30, 7, 11]]\n"
     ]
    }
   ],
   "source": [
    "sentences = re.sub(\"[.,!?\\\\-]\", '', text.lower()).split('\\n') # filter '.', ',', '?', '!'\n",
    "print(sentences)\n",
    "# set 去重， join联合， list转列表\n",
    "word_list = list(set(\" \".join(sentences).split())) # ['hello', 'how', 'are', 'you',...]\n",
    "print(\"\\nwordlist: \")\n",
    "print(word_list)\n",
    "word2idx = {'[PAD]' : 0, '[CLS]' : 1, '[SEP]' : 2, '[MASK]' : 3}\n",
    "# 为每一个词元token创建编号\n",
    "for i, w in enumerate(word_list):\n",
    "    word2idx[w] = i + 4\n",
    "print(\"\\nupdated word2dix: \")\n",
    "print(word2idx)\n",
    "idx2word = {i: w for i, w in enumerate(word2idx)}\n",
    "vocab_size = len(word2idx)\n",
    "\n",
    "token_list = list()\n",
    "for sentence in sentences:\n",
    "    arr = [word2idx[s] for s in sentence.split()]\n",
    "    token_list.append(arr)\n",
    "\n",
    "print(\"\\ntokenlist:\")\n",
    "print(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5db577a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT Parameters\n",
    "maxlen = 30  # 表示同一个 batch 中的所有句子都由 30 个 token 组成，不够的补 PAD\n",
    "batch_size = 6  # \n",
    "max_pred = 5 # max tokens of prediction 表示Bert最多需要预测多少个单词\n",
    "n_layers = 6  \n",
    "n_heads = 12\n",
    "d_model = 768  # 特征维度\n",
    "d_ff = 768*4 # 4*d_model, FeedForward dimension\n",
    "d_k = d_v = 64  # dimension of K(=Q), V\n",
    "n_segments = 2  # 表示 Decoder input 由几句话组成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c072a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample IsNext and NotNext to be same in small batch size\n",
    "def make_data():\n",
    "    batch = []\n",
    "    # pos表示Bert任务中上下文两句话是否相邻，如果是那么pos+1，否则neg+1\n",
    "    positive = negative = 0 \n",
    "    while positive != batch_size/2 or negative != batch_size/2: # 最后争取保证数量是1比1 while循环\n",
    "        # 抽取的是随机某一句话的索引，然后将两句话拼成一句话\n",
    "        tokens_a_index, tokens_b_index = randrange(len(sentences)), randrange(len(sentences)) # sample random index in sentences\n",
    "        tokens_a, tokens_b = token_list[tokens_a_index], token_list[tokens_b_index]\n",
    "        \n",
    "        # 拼接起来 加 cls和 sep\n",
    "        input_ids = [word2idx['[CLS]']] + tokens_a + [word2idx['[SEP]']] + tokens_b + [word2idx['[SEP]']]\n",
    "        segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
    "\n",
    "        # MASK LM  表示一共要做多少个mask\n",
    "        n_pred =  min(max_pred, max(1, int(len(input_ids) * 0.15))) # 15 % of tokens in one sentence\n",
    "        \n",
    "        # 记录 所有不是 special token的位置，也就是说记录原句子中单词的位置索引的list\n",
    "        cand_maked_pos = [i for i, token in enumerate(input_ids)\n",
    "                          if token != word2idx['[CLS]'] and token != word2idx['[SEP]']] # candidate masked position\n",
    "       \n",
    "        shuffle(cand_maked_pos)  # shuffle进行随机排序\n",
    "        # 也就是说可以去做mask的列表我进行随机打乱\n",
    "        \n",
    "        # 存储所有的mask的token词元 和索引\n",
    "        masked_tokens, masked_pos = [], []\n",
    "        \n",
    "        # 已经随机过了，所以取前mask数量的坐标缩进就可以了\n",
    "        for pos in cand_maked_pos[:n_pred]:\n",
    "            masked_pos.append(pos)\n",
    "            masked_tokens.append(input_ids[pos])\n",
    "            \n",
    "            # 80%的概率变成mask， 10%的概率变成另一个单词\n",
    "            if random() < 0.8:  # 80%\n",
    "                input_ids[pos] = word2idx['[MASK]'] # make mask\n",
    "            elif random() > 0.9:  # 10%\n",
    "                index = randint(0, vocab_size - 1) # random index in vocabulary\n",
    "                \n",
    "                # 不能随意替换特殊的 token_map\n",
    "                while index < 4: # can't involve 'CLS', 'SEP', 'PAD'\n",
    "                    index = randint(0, vocab_size - 1)\n",
    "                input_ids[pos] = index # replace\n",
    "\n",
    "        # Zero Paddings 如果这句话不满足maxlen 那么需要padding\n",
    "        n_pad = maxlen - len(input_ids)\n",
    "        input_ids.extend([0] * n_pad)\n",
    "        segment_ids.extend([0] * n_pad)\n",
    "\n",
    "        # Zero Padding (100% - 15%) tokens\n",
    "        if max_pred > n_pred:\n",
    "            n_pad = max_pred - n_pred\n",
    "            masked_tokens.extend([0] * n_pad)\n",
    "            masked_pos.extend([0] * n_pad)\n",
    "\n",
    "        # 这一段主要是判断ab两句话是否为前后衔接的语言，并且给出true和false\n",
    "        if tokens_a_index + 1 == tokens_b_index and positive < batch_size/2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True]) # IsNext\n",
    "            positive += 1\n",
    "        elif tokens_a_index + 1 != tokens_b_index and negative < batch_size/2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False]) # NotNext\n",
    "            negative += 1\n",
    "    return batch\n",
    "# Proprecessing Finished"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
