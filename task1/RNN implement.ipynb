{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "556b393d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 6])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(45)\n",
    "# 默认batch_size为1，只需要后续进行升维就可以unsqueeze()\n",
    "input_size, sequence_length = 6, 4 # input_szie 特征大小为6  sequence_length 词元序列长度为4\n",
    "input = torch.randn(sequence_length, input_size).unsqueeze(0) # 设置单词的长度为4，特征向量的维度是6，并且提升第0维度\n",
    "# 此时的input_size的维度为：N*L*d 对应batch_first = True\n",
    "input.shape  #N*L*d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d2164270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 初始化隐藏层\n",
    "# h_0 为初始时刻的隐状态。当RNN为单向RNN时，h_0 的形状应为 num_layers × N × h ；\n",
    "# 当RNN为双向RNN时，h_0 的形状应为 ( 2 ⋅ num_layers ) × N × h。如不提供该参数的值，则默认为全0张量。\n",
    "hidden_size = 3 \n",
    "h_prev = torch.zeros(hidden_size).unsqueeze(0)  # 这里初始化的就是单层，h层数，N样batch_size\n",
    "h_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56ee6c54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.8493, -0.5992, -0.2526],\n",
       "          [-0.6106, -0.1691,  0.5180],\n",
       "          [ 0.6820,  0.9631,  0.0999],\n",
       "          [ 0.0734, -0.5295,  0.9324]]], grad_fn=<TransposeBackward1>),\n",
       " tensor([[[ 0.0734, -0.5295,  0.9324]]], grad_fn=<StackBackward0>))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 调用pytorch的RNN api来验证结果\n",
    "rnn = nn.RNN(input_size, hidden_size, batch_first=True) # 传入两个参数：特征大小input_size和隐藏层数hidden_size\n",
    "rnn_output, state_final = rnn(input, h_prev)  # 返回两个结果，一个是output，另一个是最后时刻的h的值\n",
    "print(\"PyTorch API output:\")\n",
    "print(rnn_output)\n",
    "print(state_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1d18c707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 手写RNN_forward函数，实现RNN的计算原理（单向单层）\n",
    "def rnn_forward(input, weight_ih, bias_ih, weight_hh, bias_hh, h_prev):\n",
    "    # input的shape对应三个参数：batch_size, sequence_length, input_size\n",
    "    batch_size, sequence_length, input_size = input.shape\n",
    "    # h_dim代表隐藏层的维度，也就是隐藏层的层数 [0]是因为矩阵相乘第一个维度作为不抵消的维度\n",
    "    h_dim = weight_ih.shape[0]  \n",
    "    # 初始化一个状态（输出）矩阵\n",
    "    h_out = torch.zeros(batch_size, sequence_length, h_dim) # h_dim就是hidden_size\n",
    "    \n",
    "    for t in range(sequence_length):\n",
    "        x = input[:, t, :]  # 获取当前时刻的输入（每一个时刻对应的是一个词元向量）\n",
    "        # x 的默认大小应该是 batch_size * input_size\n",
    "        # weight_ih 的默认大小应该是 h_dim * input_size，所以要进行weight_ih的扩充，变成 batch_size*h_dim*input_size\n",
    "        weight_ih_batch = weight_ih.unsqueeze(0).tile(batch_size, 1, 1)\n",
    "        # weight_hh_batch 的大小应该是 batch_size*h_dim*h_dim\n",
    "        weight_hh_batch = weight_hh.unsqueeze(0).tile(batch_size, 1, 1)\n",
    "        # torch.bmm() 带有batch_size的矩阵相乘\n",
    "        w_times_x = torch.bmm(weight_ih_batch, x.unsqueeze(2))  #此时的x应该是 batch_size*input_size*1\n",
    "        #w_times_x 的最后的结果应该是batch_size*h_dim*1，然后可以squeeze(-1)掉\n",
    "        w_times_x = w_times_x.squeeze(-1) # batch_size*h_dim\n",
    "        \n",
    "        # h_prve最开始是batch_size*hidden_size，现在需要把它扩充到batch_size*hidden_size*1\n",
    "        w_times_h = torch.bmm(weight_hh_batch, h_prev.unsqueeze(2)).squeeze(-1)  # batch_size*h_dim \n",
    "        # 最终的计算并且不断更新\n",
    "        h_prev = torch.tanh(w_times_x + bias_ih + w_times_h + bias_hh)\n",
    "        h_out[:, t, :] = h_prev\n",
    "        \n",
    "    return h_out, h_prev.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2e1fc687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_ih_l0 Parameter containing:\n",
      "tensor([[-0.4625,  0.5091,  0.5695, -0.1479,  0.2251, -0.5359],\n",
      "        [ 0.1143, -0.0672, -0.1061, -0.5320, -0.1582, -0.4026],\n",
      "        [ 0.5427, -0.3619, -0.1699,  0.1750,  0.2764, -0.1850]],\n",
      "       requires_grad=True)\n",
      "weight_hh_l0 Parameter containing:\n",
      "tensor([[ 0.1400,  0.3478,  0.3506],\n",
      "        [-0.4183, -0.2382, -0.5081],\n",
      "        [ 0.2111,  0.0751, -0.1017]], requires_grad=True)\n",
      "bias_ih_l0 Parameter containing:\n",
      "tensor([-0.3031,  0.4574,  0.5712], requires_grad=True)\n",
      "bias_hh_l0 Parameter containing:\n",
      "tensor([ 0.3753, -0.0053, -0.3025], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 验证rnn_forward函数的准确性\n",
    "for k,v in rnn.named_parameters():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4f20e0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnn_forward output:\n",
      "tensor([[[ 0.8493, -0.5992, -0.2526],\n",
      "         [-0.6106, -0.1691,  0.5180],\n",
      "         [ 0.6820,  0.9631,  0.0999],\n",
      "         [ 0.0734, -0.5295,  0.9324]]], grad_fn=<CopySlices>)\n",
      "tensor([[[ 0.0734, -0.5295,  0.9324]]], grad_fn=<UnsqueezeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "custom_rnn_output, custom_state_fina = rnn_forward(input, rnn.weight_ih_l0, rnn.bias_ih_l0, rnn.weight_hh_l0, rnn.bias_hh_l0, h_prev)\n",
    "print(\"rnn_forward output:\")\n",
    "print(custom_rnn_output)\n",
    "print(custom_state_fina)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5c1dd84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bidirectional_rnn_forward 函数手写实现，实现双向RNN计算\n",
    "def bi_rnn_forward(input, weight_ih, bias_ih, weight_hh, bias_hh, h_prev, \\\n",
    "                   weight_ih_reverse, weight_hh_reverse, bias_ih_reverse, bias_hh_reverse, h_prev_reverse):\n",
    "    # input的shape对应三个参数：batch_size, sequence_length, input_size\n",
    "    batch_size, sequence_length, input_size = input.shape\n",
    "    # h_dim代表隐藏层的维度，也就是隐藏层的层数 [0]是因为矩阵相乘第一个维度作为不抵消的维度\n",
    "    h_dim = weight_ih.shape[0]  \n",
    "    # 初始化一个状态（输出）矩阵\n",
    "    h_out = torch.zeros(batch_size, sequence_length, h_dim*2) # h_dim就是hidden_size，现在应该是双层结构，所以是两倍\n",
    "    \n",
    "    \n",
    "    # 两层都可以调用我们自己写的rnn_forward() \n",
    "    forward_output = rnn_forward(input, weight_ih, bias_ih, weight_hh, bias_hh, h_prev)[0]  # forward_layer\n",
    "    # backward_layer 的input应该是相较于forward反向过来，具体的反向是input中的sequence（词元）进行反向\n",
    "    backward_output = rnn_forward(torch.flip(input, [1]), weight_ih_reverse, bias_ih_reverse, \\\n",
    "                                   weight_hh_reverse, bias_hh_reverse, h_prev_reverse)[0]\n",
    "    \n",
    "    h_out[:, :, :h_dim] = forward_output\n",
    "    h_out[:, :, h_dim:] = torch.flip(backward_output, [1])  #这里需要重新反转才能和forward_layout进行拼接\n",
    "    \n",
    "    return h_out, h_out[:, -1, :].reshape((batch_size, 2, h_dim)).transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6f6735cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch API output:\n",
      "tensor([[[-0.6087,  0.0795,  0.3462,  0.9711, -0.9796, -0.6105],\n",
      "         [ 0.2244, -0.6353,  0.0294, -0.1929, -0.9441,  0.5532],\n",
      "         [-0.7982, -0.6700, -0.9200, -0.1233,  0.6410, -0.3867],\n",
      "         [ 0.4198,  0.8892,  0.4646,  0.0647, -0.9828, -0.9097]]],\n",
      "       grad_fn=<TransposeBackward1>)\n",
      "tensor([[[ 0.4198,  0.8892,  0.4646]],\n",
      "\n",
      "        [[ 0.9711, -0.9796, -0.6105]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 验证bi_rnn_forward() 函数的正确性\n",
    "# 首先重新实例化一个RNN\n",
    "bi_rnn = nn.RNN(input_size, hidden_size, batch_first=True, bidirectional=True) # 传入两个参数：特征大小input_size和隐藏层数hidden_size\n",
    "batch_size = 1  # 样本大小目前给的就是1\n",
    "h_prev = torch.zeros(2, batch_size, hidden_size)\n",
    "bi_rnn_output, bi_state_final = bi_rnn(input, h_prev)\n",
    "print(\"PyTorch API output:\")\n",
    "print(bi_rnn_output)\n",
    "print(bi_state_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e2134647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_ih_l0 Parameter containing:\n",
      "tensor([[ 0.5296,  0.0321,  0.1165, -0.2557,  0.2418,  0.4144],\n",
      "        [-0.5435,  0.1551, -0.3544,  0.3543,  0.4802, -0.4232],\n",
      "        [ 0.4194,  0.3039, -0.3030,  0.2087,  0.3193,  0.2257]],\n",
      "       requires_grad=True)\n",
      "weight_hh_l0 Parameter containing:\n",
      "tensor([[-0.5647, -0.2097, -0.4985],\n",
      "        [-0.5652,  0.1587, -0.0429],\n",
      "        [ 0.2237,  0.5574,  0.0811]], requires_grad=True)\n",
      "bias_ih_l0 Parameter containing:\n",
      "tensor([-0.0479, -0.0370,  0.2689], requires_grad=True)\n",
      "bias_hh_l0 Parameter containing:\n",
      "tensor([-0.4902,  0.0525, -0.2702], requires_grad=True)\n",
      "weight_ih_l0_reverse Parameter containing:\n",
      "tensor([[-0.3424,  0.4879,  0.3612,  0.3389,  0.0303, -0.0915],\n",
      "        [-0.5010, -0.4015,  0.0525, -0.4627, -0.5156, -0.5086],\n",
      "        [-0.3423, -0.5647,  0.5610, -0.3788,  0.1533,  0.3255]],\n",
      "       requires_grad=True)\n",
      "weight_hh_l0_reverse Parameter containing:\n",
      "tensor([[-0.3206, -0.3444, -0.0685],\n",
      "        [ 0.5382,  0.1234,  0.4068],\n",
      "        [-0.0759,  0.3591,  0.5213]], requires_grad=True)\n",
      "bias_ih_l0_reverse Parameter containing:\n",
      "tensor([ 0.0982, -0.5127,  0.0515], requires_grad=True)\n",
      "bias_hh_l0_reverse Parameter containing:\n",
      "tensor([-0.1243, -0.4845,  0.0124], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for k,v in bi_rnn.named_parameters():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4eca29f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bi_rnn_forward output:\n",
      "tensor([[[-0.6087,  0.0795,  0.3462,  0.9711, -0.9796, -0.6105],\n",
      "         [ 0.2244, -0.6353,  0.0294, -0.1929, -0.9441,  0.5532],\n",
      "         [-0.7982, -0.6700, -0.9200, -0.1233,  0.6410, -0.3867],\n",
      "         [ 0.4198,  0.8892,  0.4646,  0.0647, -0.9828, -0.9097]]],\n",
      "       grad_fn=<CopySlices>)\n",
      "tensor([[[ 0.4198,  0.8892,  0.4646]],\n",
      "\n",
      "        [[ 0.0647, -0.9828, -0.9097]]], grad_fn=<TransposeBackward0>)\n",
      "PyTorch API output:\n",
      "tensor([[[-0.6087,  0.0795,  0.3462,  0.9711, -0.9796, -0.6105],\n",
      "         [ 0.2244, -0.6353,  0.0294, -0.1929, -0.9441,  0.5532],\n",
      "         [-0.7982, -0.6700, -0.9200, -0.1233,  0.6410, -0.3867],\n",
      "         [ 0.4198,  0.8892,  0.4646,  0.0647, -0.9828, -0.9097]]],\n",
      "       grad_fn=<TransposeBackward1>)\n",
      "tensor([[[ 0.4198,  0.8892,  0.4646]],\n",
      "\n",
      "        [[ 0.9711, -0.9796, -0.6105]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "custom_bi_rnn_output, custom_bi_state_final = bi_rnn_forward(input, bi_rnn.weight_ih_l0, bi_rnn.bias_ih_l0, \\\n",
    "                                                            bi_rnn.weight_hh_l0, bi_rnn.bias_hh_l0, h_prev[0], \\\n",
    "                                                            bi_rnn.weight_ih_l0_reverse, bi_rnn.weight_hh_l0_reverse, \\\n",
    "                                                            bi_rnn.bias_ih_l0_reverse, bi_rnn.bias_hh_l0_reverse, h_prev[1])\n",
    "print(\"bi_rnn_forward output:\")\n",
    "print(custom_bi_rnn_output)\n",
    "print(custom_bi_state_final)\n",
    "print(\"PyTorch API output:\")\n",
    "print(bi_rnn_output)\n",
    "print(bi_state_final)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
